"""
æ·±ã•åˆ¶é™ä»˜ãCoach

alpha-zero-generalã®Coachã‚’æ‹¡å¼µã—ã¦ã€DepthLimitedMCTSã‚’ä½¿ç”¨
"""

import os
import sys
import numpy as np
import logging
from collections import deque
from random import shuffle
from pickle import Pickler, Unpickler

# alpha-zero-generalã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'alpha-zero-general'))

from Coach import Coach
from DepthLimitedMCTS import DepthLimitedMCTS

log = logging.getLogger(__name__)


class DepthLimitedCoach(Coach):
    """
    æ·±ã•åˆ¶é™ä»˜ãMCTSã‚’ä½¿ç”¨ã™ã‚‹Coachã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, game, nnet, args):
        """
        Args:
            game: ã‚²ãƒ¼ãƒ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            nnet: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            args: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–ã®å‰ã«ã€argsã‚’è¨­å®š
        self.game = game
        self.nnet = nnet
        self.args = args
        self.pnet = self.nnet.__class__(self.game, self.args)  # previous net
        self.trainExamplesHistory = []
        self.skipFirstSelfPlay = False
        
        # MCTSã‚’æ·±ã•åˆ¶é™ä»˜ãMCTSã§ä½œæˆï¼ˆè¦ªã®MCTSã‚’ä½¿ã‚ãªã„ï¼‰
        self.mcts = DepthLimitedMCTS(self.game, self.nnet, self.args)
        
        print(f"âœ… æ·±ã•åˆ¶é™ä»˜ãMCTSä½œæˆå®Œäº†")
        print(f"   æœ€å¤§æ·±ã•: {self.mcts.max_depth}")
    
    def executeEpisode(self):
        """
        1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è‡ªå·±å¯¾æˆ¦ã‚’å®Ÿè¡Œ
        
        Returns:
            trainExamples: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ [(board, pi, v), ...]
        """
        trainExamples = []
        board = self.game.getInitBoard()
        self.curPlayer = 1
        episodeStep = 0
        
        # MCTSãŒæ­£ã—ã„å‹ã‹ç¢ºèª
        if not isinstance(self.mcts, DepthLimitedMCTS):
            print(f"âš ï¸ MCTSãŒç½®ãæ›ãˆã‚‰ã‚Œã¦ã„ã¾ã™: {type(self.mcts)}")
            # å¼·åˆ¶çš„ã«DepthLimitedMCTSã«ç½®ãæ›ãˆ
            self.mcts = DepthLimitedMCTS(self.game, self.nnet, self.args)
            print(f"âœ… DepthLimitedMCTSã§å†ä½œæˆã—ã¾ã—ãŸ")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã«çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ
        self.mcts.reset_stats()

        while True:
            episodeStep += 1
            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)
            temp = int(episodeStep < self.args.tempThreshold)

            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)
            
            # ãƒ‡ãƒãƒƒã‚°: piã®ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®æ‰‹ã®ã¿ï¼‰
            if episodeStep == 1:
                print(f"   ãƒ‡ãƒãƒƒã‚°: pi shape = {len(pi)}, æœŸå¾…å€¤ = {self.game.getActionSize()}")
            
            sym = self.game.getSymmetries(canonicalBoard, pi)
            for b, p in sym:
                trainExamples.append([b, self.curPlayer, p, None])

            action = np.random.choice(len(pi), p=pi)
            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)

            r = self.game.getGameEnded(board, self.curPlayer)

            if r != 0:
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
                # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
                stats = self.mcts.get_stats()
                print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†: {episodeStep}æ‰‹")
                print(f"   æœ€å¤§æ¢ç´¢æ·±ã•: {stats['max_depth_reached']}")
                print(f"   æ·±ã•åˆ¶é™åˆ°é”å›æ•°: {stats['depth_limit_hits']}")
                print(f"   æ¢ç´¢çŠ¶æ…‹æ•°: {stats['total_states']}")
                
                return [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]
            
            # å®‰å…¨ã®ãŸã‚ã€æœ€å¤§æ‰‹æ•°åˆ¶é™
            if episodeStep > 200:
                print(f"   âš ï¸ æœ€å¤§æ‰‹æ•°åˆ°é”ï¼ˆ200æ‰‹ï¼‰ã€å¼•ãåˆ†ã‘ã¨ã—ã¦çµ‚äº†")
                return [(x[0], x[2], 0) for x in trainExamples]
    
    def learn(self):
        """
        å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œï¼ˆè©•ä¾¡çµæœã®è¡¨ç¤ºã‚’è¿½åŠ ï¼‰
        
        å…ƒã®Coachã®learn()ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€è©•ä¾¡çµæœã®è¡¨ç¤ºã‚’æ”¹å–„
        """
        from Arena import Arena
        from MCTS import MCTS
        from tqdm import tqdm
        
        for i in range(1, self.args.numIters + 1):
            # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹
            log.info(f'Starting Iter #{i} ...')
            print(f"\n{'='*70}")
            print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {i}/{self.args.numIters}")
            print(f"{'='*70}")
            
            # Self-play (è‡ªå·±å¯¾æˆ¦)
            if not self.skipFirstSelfPlay or i > 1:
                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)
                
                for _ in tqdm(range(self.args.numEps), desc="Self Play"):
                    self.mcts = DepthLimitedMCTS(self.game, self.nnet, self.args)  # reset search tree
                    iterationTrainExamples += self.executeEpisode()
                
                # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ’ã‚¹ãƒˆãƒªãƒ¼ã«è¿½åŠ 
                self.trainExamplesHistory.append(iterationTrainExamples)
            
            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:
                log.warning(
                    f"Removing the oldest entry in trainExamples. len(trainExamplesHistory) = {len(self.trainExamplesHistory)}")
                self.trainExamplesHistory.pop(0)
            
            # å­¦ç¿’ä¾‹ã‚’ä¿å­˜
            self.saveTrainExamples(i)
            
            # å…¨ã¦ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦è¨“ç·´
            trainExamples = []
            for e in self.trainExamplesHistory:
                trainExamples.extend(e)
            shuffle(trainExamples)
            
            # temp.pth.tarã«ä¿å­˜ï¼ˆè©•ä¾¡å‰ã®æ—§ãƒ¢ãƒ‡ãƒ«ï¼‰
            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            pmcts = DepthLimitedMCTS(self.game, self.pnet, self.args)
            
            # æ–°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
            self.nnet.train(trainExamples)
            nmcts = DepthLimitedMCTS(self.game, self.nnet, self.args)
            
            # è©•ä¾¡ï¼šæ–°ãƒ¢ãƒ‡ãƒ« vs æ—§ãƒ¢ãƒ‡ãƒ«
            log.info('PITTING AGAINST PREVIOUS VERSION')
            print(f"\n{'='*70}")
            print("è©•ä¾¡å¯¾æˆ¦: æ–°ãƒ¢ãƒ‡ãƒ« vs æ—§ãƒ¢ãƒ‡ãƒ«")
            print(f"{'='*70}")
            
            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),
                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)
            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)
            
            # çµæœè¡¨ç¤ºï¼ˆç›®ç«‹ã¤ã‚ˆã†ã«ï¼‰
            print(f"\n{'='*70}")
            print(f"ğŸ“Š è©•ä¾¡çµæœ")
            print(f"{'='*70}")
            print(f"  æ–°ãƒ¢ãƒ‡ãƒ«ã®å‹åˆ©: {nwins}")
            print(f"  æ—§ãƒ¢ãƒ‡ãƒ«ã®å‹åˆ©: {pwins}")
            print(f"  å¼•ãåˆ†ã‘: {draws}")
            if pwins + nwins > 0:
                win_rate = float(nwins) / (pwins + nwins)
                print(f"  æ–°ãƒ¢ãƒ‡ãƒ«ã®å‹ç‡: {win_rate:.1%}")
            print(f"{'='*70}\n")
            
            log.info('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))
            
            # ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã®åˆ¤å®š
            if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.args.updateThreshold:
                log.info('REJECTING NEW MODEL')
                print(f"âŒ æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ä¸æ¡ç”¨ï¼ˆå‹ç‡ãŒé–¾å€¤ {self.args.updateThreshold:.0%} æœªæº€ï¼‰\n")
                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            else:
                log.info('ACCEPTING NEW MODEL')
                print(f"âœ… æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ï¼best.pth.tarã‚’æ›´æ–°\n")
                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))
                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')
    
    def getCheckpointFile(self, iteration):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ"""
        return 'checkpoint_' + str(iteration) + '.pth.tar'
    
    def saveTrainExamples(self, iteration):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        folder = self.args.checkpoint
        if not os.path.exists(folder):
            os.makedirs(folder)
        filename = os.path.join(folder, self.getCheckpointFile(iteration) + ".examples")
        with open(filename, "wb+") as f:
            Pickler(f).dump(self.trainExamplesHistory)
        f.closed

